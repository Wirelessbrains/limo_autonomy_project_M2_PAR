%========================================================
% Classe do documento
%========================================================
\documentclass[a4paper,11pt]{article}
\PassOptionsToPackage{table,xcdraw}{xcolor}

%========================================================
% Idioma e fontes
%========================================================
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{latexsym}
\usepackage{indentfirst}
\usepackage{mathpazo}
\usepackage[scaled]{helvet}
\linespread{1.05}

%========================================================
% Layout de página e cabeçalho/rodapé
% (Ajuste de parágrafos mais “relatório”: menos espaçado e indent menor)
%========================================================
\usepackage[
left=2cm,right=2cm,bottom=2cm,top=2cm,
headheight=2cm,headsep=0.5cm,heightrounded=true
]{geometry}

\setlength{\parskip}{0.5em}
\setlength{\parindent}{1.5em}
\setlength{\emergencystretch}{1em}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\sectionmark}[1]{\markboth{#1}{}}
\fancyhead[L]{\small \nouppercase{\leftmark}}
\fancyhead[R]{\small AprilTag-based Trajectory Tracking}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

%========================================================
% Links, metadados PDF e estrutura (TOC e títulos)
% (Remove hack do \vspace dentro do título do TOC e remove redefinição de \section)
%========================================================
\usepackage{xpatch}
\usepackage{float}
\usepackage{hyperref}
\usepackage{caption}
\usepackage[list=true]{subcaption}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\usepackage{chngcntr}
\usepackage{tocloft}
\usepackage{titlesec}

% --- Figuras e TikZ
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}

% --- Algoritmo (pseudocódigo)
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

% --- Referências clicáveis (opcional, mas bom)
\usepackage{hyperref}


\renewcommand{\contentsname}{Table of Contents}

% Espaço antes do título do sumário (estável, sem “hack”)
\setlength{\cftbeforetoctitleskip}{4cm}
\renewcommand{\cfttoctitlefont}{\normalfont\huge\bfseries}
\setlength{\cftaftertoctitleskip}{1.0cm}

\renewcommand{\cftsecfont}{\bfseries}
\renewcommand{\cftsecpagefont}{\bfseries}
\setlength{\cftbeforesecskip}{1.2em}
\setlength{\cftbeforesubsecskip}{0.3em}
\setlength{\cftbeforesubsubsecskip}{0.0em}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\titleformat{\section}{\normalfont\huge\bfseries}{\thesection}{1em}{}
\titlespacing*{\section}{0pt}{1.2cm}{0.8cm}

\titleformat{\subsection}{\normalfont\Large\bfseries}{\thesubsection}{1em}{}
\titlespacing*{\subsection}{0pt}{1.0cm}{0.2cm}

\titleformat{\subsubsection}{\normalfont\large\bfseries}{\thesubsubsection}{1em}{}
\titlespacing*{\subsubsection}{0pt}{0.5cm}{0.2cm}

% Se você quiser seção sempre em nova página, use \newsection{...} (mais seguro)
\newcommand{\newsection}[1]{\clearpage\section{#1}}

\hypersetup{
	pdftitle={Title},
	pdfsubject={Subject},
	pdfauthor={Author},
	pdfkeywords={keyword1} {keyword2} {keyword3}
}

%========================================================
% Matemática, símbolos, caixas e utilitários de texto
%========================================================
\usepackage{xcolor}
\usepackage{bm}
\usepackage{ragged2e}
\usepackage{tcolorbox}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{enumitem}

%========================================================
% Listas e marcadores
%========================================================
\definecolor{bulletcolor}{RGB}{128,128,128}

\setenumerate[1]{label=\Alph*.}
\setenumerate[2]{label=\arabic*.}
\setenumerate[3]{label=\roman*.}

\setitemize[1]{label=\ding{222}, font=\LARGE \color{bulletcolor}}
\setitemize[2]{label=\textbullet, font=\LARGE \color{bulletcolor}}
\setitemize[3]{label=$\triangleright$, font=\LARGE \color{bulletcolor}}

%========================================================
% Unidades (SI)
%========================================================
\usepackage{siunitx}
\newcommand{\nomunit}[1]{\renewcommand{\nomentryend}{\hspace*{\fill}#1}}
\sisetup{inter-unit-product=\ensuremath{{}\cdot{}}}
\DeclareSIUnit\bar{bar}
\DeclareSIPower\quartic\tothefourth{4}

%========================================================
% Sublinhado com contorno (myuline)
%========================================================
\usepackage{contour}
\usepackage[normalem]{ulem}
\usepackage{xparse}

\renewcommand{\ULdepth}{1.8pt}
\contourlength{0.8pt}

\newcommand{\myuline}[1]{%
	\uline{\phantom{#1}}%
	\llap{\contour{white}{#1}}%
}

\ExplSyntaxOn
\NewDocumentCommand{\myulineX}{m}
{
	\seq_set_split:Nnn \l_tmpa_seq { ~ } { #1 }
	\seq_map_inline:Nn \l_tmpa_seq { \myuline{##1} ~ } \unskip
}
\ExplSyntaxOff



%========================================================
% Nomenclatura
%========================================================
\usepackage[intoc]{nomencl}
\usepackage{etoolbox}
\makenomenclature

\renewcommand\nomgroup[1]{%
	\item[\bfseries
	\ifstrequal{#1}{A}{Modelo de Câmera}{%
		\ifstrequal{#1}{B}{Geometria e Transformações}{%
			\ifstrequal{#1}{C}{AprilTag e Algoritmos}{%
	}}}%
	]}

\AtBeginDocument{
	\nomenclature[A]{$K$}{Matriz intrínseca da câmera (parâmetros internos)}
	\nomenclature[A]{$f_x, f_y$}{Distâncias focais expressas em pixels}
	\nomenclature[A]{$(c_x, c_y)$}{Coordenadas do ponto principal da imagem}
	\nomenclature[A]{$(u, v)$}{Coordenadas de pixel na imagem}
	\nomenclature[A]{$\mathbf{p}_c$}{Ponto 3D expresso no referencial da câmera $[X_c\;Y_c\;Z_c]^\top$}
	\nomenclature[A]{$(x_n, y_n)$}{Coordenadas normalizadas no plano de imagem}
	\nomenclature[A]{$(x_d, y_d)$}{Coordenadas de imagem distorcidas}
	\nomenclature[A]{$k_i, p_i$}{Coeficientes de distorção radial e tangencial}
	\nomenclature[A]{$\mathbf{u}_i$}{Coordenadas de pixel observadas (medição)}
	\nomenclature[A]{$\boldsymbol{\epsilon}_i$}{Ruído de medição associado à observação de cantos}
	\nomenclature[A]{$\Sigma_u$}{Matriz de covariância do ruído de medição}
	\nomenclature[B]{${}^{A}T_{B}$}{Matriz de transformação homogênea do referencial B para A}
	\nomenclature[B]{$R$}{Matriz de rotação $SO(3)$}
	\nomenclature[B]{$t$}{Vetor de translação $\mathbb{R}^3$}
	\nomenclature[C]{$d_H(a,b)$}{Distância de Hamming entre dois códigos binários}
	\nomenclature[C]{$H$}{Matriz de homografia planar}
}

%========================================================
% Figuras
%========================================================
\usepackage{graphicx}
\graphicspath{{Images/}}

\counterwithin{figure}{section}
\setcounter{lofdepth}{2}
\cftsetindents{figure}{0em}{3.5em}
\setlength\cftbeforefigskip{5pt}
\AtBeginDocument{\renewcommand{\listfigurename}{List of Figures}}
\xpretocmd{\listoffigures}{\addcontentsline{toc}{section}{\listfigurename}}{}{}

%========================================================
% Tabelas
%========================================================
\usepackage{array,multirow,makecell}
\setcellgapes{1pt}
\makegapedcells
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash }b{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash }b{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash }b{#1}}

\counterwithin{table}{section}
\setcounter{lotdepth}{2}
\cftsetindents{table}{0em}{3.5em}
\setlength\cftbeforetabskip{5pt}
\xpretocmd{\listoftables}{\addcontentsline{toc}{section}{\listtablename}}{}{}

%========================================================
% Lista de Equações
%========================================================
\newcommand{\listequationsname}{List of Equations}
\newlistof{myequations}{equ}{\listequationsname}

\newcommand{\myequations}[1]{%
	\addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}
}

\counterwithin{equation}{section}
\cftsetindents{myequations}{0em}{3.5em}
\setlength\cftbeforemyequationsskip{5pt}

\newcommand{\noteworthy}[2]{%
	\begin{align}\label{#2}\ensuremath{\boxed{#1}}\end{align}
	\myequations{#2}
	\begingroup
	\centering \small \textit{#2}
	\endgroup
}

\xpretocmd{\listofmyequations}{\addcontentsline{toc}{section}{\listequationsname}}{}{}

%========================================================
% Código-fonte (listings)
% (Inclui também a entrada da lista de listings no TOC)
%========================================================
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}

\lstset{style=mystyle}
\renewcommand{\lstlistingname}{Listing}
\renewcommand{\lstlistlistingname}{List of Listings}
\xpretocmd{\lstlistoflistings}{\addcontentsline{toc}{section}{\lstlistlistingname}}{}{}

%========================================================
% Índice remissivo
%========================================================
\usepackage{imakeidx}
\makeindex
\xpretocmd{\printindex}{\addcontentsline{toc}{section}{\indexname}}{}{}

%========================================================
% Bibliografia (biblatex)
% (Sugestão: em relatório, normalmente REMOVER \nocite{*})
%========================================================
\usepackage[backend=biber,style=ieee,citestyle=ieee,block=ragged,sorting=none]{biblatex}
\usepackage{csquotes}
\addbibresource{biblio.bib}
% \nocite{*}

%========================================================
% Anexos e PDFs
%========================================================
\usepackage[toc,page,title,titletoc,header]{appendix}
\usepackage{pdfpages}
\usepackage{titletoc}

\renewcommand{\appendixtocname}{List of Appendices}
\renewcommand{\appendixpagename}{Appendices}

%========================================================
% Utilitários
%========================================================
\usepackage[all,defaultlines=3]{nowidow}
\usepackage{blindtext}

%========================================================
% Início do documento
%========================================================
\begin{document}
	
	\hypersetup{pageanchor=false}
	\begin{titlepage}
		\begin{center}
			\includegraphics[scale=0.16]{uca.png}\hfill
			\includegraphics[scale=0.02]{eupi_long.png}
			\\[2cm]
			
			{\Large \textsc{Université Clermont Auvergne}}\\[0.5cm]
			{\large École Universitaire de Physique et d'Ingénierie}
			
			\vfill
			
			\rule{\linewidth}{0.5mm}\\[0.4cm]
			{\huge \bfseries AprilTag-based Trajectory Tracking\\[0.2cm] for the LIMOS Robot}\\[0.4cm]
			\rule{\linewidth}{0.5mm}\\[1.5cm]
			
			{\Large Second-year Master's project}\\[0.5cm]
			{\large Automation, Robotics track: Artificial Perception and Robotics}
			
			\vfill
			
			\begin{minipage}{0.45\textwidth}
				\begin{flushleft}\large
					\emph{Authors:}\\
					Joao Pedro \textsc{Martins do Lago Reis}\\
					Yann Kelvem \textsc{Da Silva Ramos}
				\end{flushleft}
			\end{minipage}
			\hfill
			\begin{minipage}{0.45\textwidth}
				\begin{flushright}\large
					\emph{Supervisor:}\\
					Dr. Sébastien \textsc{Lengagne}\\
					\vspace{0.5cm}
					\emph{Defense Date:}\\
					March 02, 2026
				\end{flushright}
			\end{minipage}
			
			\vfill
			Aubière, Clermont-Ferrand
		\end{center}
	\end{titlepage}
	
	\newpage
	\thispagestyle{empty}
	\begin{center}
		\Large \textbf{Title}
		
		\vspace{0.4cm}
		\large Subtitle
		
		\vspace{0.4cm}
		\textbf{Author}
		
		\vspace{0.9cm}
		\textbf{Abstract}
	\end{center}
	
	\blindtext
	
	%========================================================
	% TOC & listas (relatório “completo”)
	%========================================================
	\newpage
	\hypersetup{pageanchor=true}
	\setcounter{page}{1}
	
	\tableofcontents
	\newpage
	\listoffigures


	
	\newpage
	\printnomenclature

	


\newpage
\section{Introduction}
	Being able to follow a path once and repeat it optimally is a practical skill for mobile robots, especially in indoor environments where GPS is not available and tasks are repetitive by nature. Thus, this is the central idea behind Teach and Replay\cite{nourizadeh2024teachrepeatnavigationrobust}, the robot first performs a guided route while it registers the markers looking for a positioning reference of each tag and then uses this record of positions to reproduce the same route optimally.

	In this practical exercise, this concept was implemented on a LIMO mobile robot using its integrated RGB cam and AprilTag \cite{olson2011tags} fiducial markers positioned along a closed circuit with a start and end. During the teach phase, the robot completes a first turn while detecting the markers and saves the information from the positions of the robot in relation to the tags to represent the executed movement. After pressing the Y key of the Xbox controller or another controller's triangle, the system switches to the replay phase, where the robot tries to follow the learned trajectory. The goal is simple: complete the circuit, reducing the path deviation and maintaining a safe distance from the tags, but a restriction is imposed on the user of at least always seeing two tags in the cam.
	
	In order to develop the solution in a controlled way and evaluate its behavior in real conditions, a simulation workflow was made for reality. Therefore, it started in the Gazebo Ignition, where the perception and control components have been tested  and validated. We then transfer the same workflow to the real LIMO robot, where external noise such as lighting variation, motion blur, and wheel slippage naturally increase the challenge of detection quality for tracking performance. This report therefore documents not only the final system but also the changes that occurred when moving from simulation to a real platform.
	
	The rest of the report presents the requirements of the problem, justifying that it will be useful a tracking system trajectory, and also details the methodology proposed for the method Teach and Replay, both for the part of the validation in simulation and for the implementation in the real case, defines the evaluation metrics and discusses the results obtained with a direct comparison between simulated and real executions.
	
	However, this work was that it will serve to help students from related areas of programming and robotics at Polytechnique de Clermont-Ferrand to use the system in a practical work, so they can understand how the simple camera perception part, using markers, can be applied directly to mobile robotics, understand the complexity of the transformations of positions used and how, with a simple camera and markers, it is possible to trace a trajectory and, with optimization, arrive at the best possible trajectory.

\subsection{Objectives}

\subsubsection{General objective}

To develop and validate a vision-based \textit{Teach and Replay} navigation system for a mobile robot, using an RGB camera and fiducial markers, capable of learning and reproducing a trajectory.

\subsubsection{Specific objectives}
\begin{itemize}
	\item Implement a visual perception pipeline based on AprilTag detection for estimating the relative pose between the robot and multiple observed markers.
	
	\item Design and implement the \textit{teach} phase in a closed circuit, recording the guided trajectory of the robot under the operational constraint of simultaneous visibility of at least two markers.
	
	\item Develop the \textit{replay} phase, allowing the reproduction of the learned trajectory with minimization of path deviation.
	
	\item Validate and debug the perception and control modules in a simulation environment, under controlled conditions, before implementation on real hardware.
	
	\item Transfer the same workflow to the LIMO mobile robot and analyze the impact of real-world disturbances.
	
	\item Make available a didactic and reusable system on GitHub.
	
\end{itemize}

\section{Theoretical foundation}

\subsection{Pose composition in \texorpdfstring{$SE(3)$}{SE(3)}}
\label{subsubsec:se3_comp}

In mobile robotics and computer vision, the geometric relationship between two rigid reference frames is described by a transformation of the special Euclidean group $SE(3)$, which combines rotation and translation into a single homogeneous matrix \cite{hartley2004mv,barfoot2017state}. Thus, the pose of a reference frame $B$ with respect to a reference frame $A$ is represented by:
\begin{equation}
	{}^{A}\!T_{B}=
	\begin{bmatrix}
		{}^{A}\!R_{B} & {}^{A}\!t_{B}\\
		0 & 1
	\end{bmatrix},
	\qquad {}^{A}\!R_{B}\in SO(3),\; {}^{A}\!t_{B}\in\mathbb{R}^3,
	\label{eq:se3}
\end{equation}
In which ${}^{A}\!R_{B}$ encodes the orientation of $B$ expressed in $A$, and ${}^{A}\!t_{B}$ is the position vector of the origin of $B$ expressed in $A$. Equation~\eqref{eq:se3} provides the transformation for a 3D point $\mathbf{p}_B$ expressed in $B$, obtaining its coordinate in $A$ by:
\begin{equation}
	\bar{\mathbf{p}}_A = {}^{A}\!T_{B}\,\bar{\mathbf{p}}_B,
	\qquad
	\bar{\mathbf{p}}=\begin{bmatrix}\mathbf{p}\\1\end{bmatrix}.
	\label{eq:se3_point}
\end{equation}

Thus, the composition of poses follows directly from the product of homogeneous matrices. In particular, if ${}^{A}\!T_{B}$ represents the pose of $B$ with respect to $A$ and ${}^{B}\!T_{C}$ the pose of $C$ with respect to $B$, then the pose of $C$ with respect to $A$ is given as:
\begin{equation}
	{}^{A}\!T_{C}=
	{}^{A}\!T_{B}\;{}^{B}\!T_{C},
	\label{eq:se3_chain}
\end{equation}

which allows chaining relative measurements and absolute reference frames within the same geometric model \cite{hartley2004mv,barfoot2017state}.


Therefore, three reference frames were adopted, the camera as \textit{cam}, the marker as \textit{tag}, and a global reference frame as \textit{world}. Visual estimation directly provides the transformation of the marker with respect to the camera, denoted by ${}^{\text{cam}}\!T_{\text{tag}}$, that is, the pose of the \textit{tag} expressed in the camera reference frame, in the form of Equation~\eqref{eq:se3}. When necessary, the pose of the camera with respect to the marker is obtained by the inverse:
\begin{equation}
	{}^{\text{tag}}\!T_{\text{cam}}=\left({}^{\text{cam}}\!T_{\text{tag}}\right)^{-1}.
	\label{eq:se3_inverse_cam_tag}
\end{equation}
In addition, transformations between camera, marker, and world are obtained by chaining according to Equation~\eqref{eq:se3_chain}. For example, if ${}^{\text{world}}\!T_{\text{tag}}$ is known and ${}^{\text{cam}}\!T_{\text{tag}}$ is measured by the camera, its pose in the world can be written as:
\begin{equation}
	{}^{\text{world}}\!T_{\text{cam}}
	=
	{}^{\text{world}}\!T_{\text{tag}}\;
	{}^{\text{tag}}\!T_{\text{cam}}
	=
	{}^{\text{world}}\!T_{\text{tag}}\;
	\left({}^{\text{cam}}\!T_{\text{tag}}\right)^{-1}.
	\label{eq:world_T_cam}
\end{equation}
This convention will be used to combine successive observations of multiple tags and express all poses in a global reference frame, being necessary to avoid unnecessary calculations and transformations.


\subsection{Camera}
\label{subsec:camera}

The geometric modeling presented in Section~\ref{subsubsec:se3_comp} allows expressing 3D points in different reference frames through transformations in $SE(3)$. 
In particular, a point $\mathbf{P}_w \in \mathbb{R}^3$ expressed in the global reference frame can be converted to the camera reference frame, thus we have that:

\begin{equation}
	\bar{\mathbf{P}}_{c}
	=
	{}^{\text{cam}}\!T_{\text{world}}\;\bar{\mathbf{P}}_{w},
	\qquad
	\bar{\mathbf{P}}=\begin{bmatrix}\mathbf{P}\\1\end{bmatrix},
	\label{eq:world_to_cam_point}
\end{equation}

In which ${}^{\text{cam}}\!T_{\text{world}}\in SE(3)$ represents the pose of the \textit{world} reference frame expressed in the camera reference frame, according to the convention of Equation~\eqref{eq:se3}. 
Equivalently, explicitly writing rotation and translation,
\begin{equation}
	\mathbf{P}_c
	=
	{}^{\text{cam}}\!R_{\text{world}}\,\mathbf{P}_w
	+
	{}^{\text{cam}}\!t_{\text{world}},
	\label{eq:world_to_cam_rt}
\end{equation}
the point in the camera reference frame $\mathbf{P}_c=[X_c\;Y_c\;Z_c]^\top$ is obtained, which is the point used by the \textit{pinhole} projective model \cite{hartley2004mv}.


Therefore, considering a 3D point expressed in the camera reference frame $\mathbf{P}_c=[X_c\;Y_c\;Z_c]^\top$, with $Z_c>0$, its coordinates in the normalized image plane are given by:

\begin{equation}
	x_n=\frac{X_c}{Z_c},
	\qquad
	y_n=\frac{Y_c}{Z_c}.
	\label{eq:normalized_coords}
\end{equation}

With this, the conversion of these normalized coordinates to pixel coordinates is performed by the intrinsic matrix $K$, which incorporates the focal length and the principal point. Thus, the projection in pixels can be written as follows:

\begin{equation}
	\begin{bmatrix}
		u\\v\\1
	\end{bmatrix}
	=
	K
	\begin{bmatrix}
		x_n\\y_n\\1
	\end{bmatrix},
	\qquad
	K=
	\begin{bmatrix}
		f_x & 0   & c_x\\
		0   & f_y & c_y\\
		0   & 0   & 1
	\end{bmatrix},
	\label{eq:intrinsics}
\end{equation}

in which $f_x$ and $f_y$ are the focal lengths in pixels and $(c_x,c_y)$ is the principal point.

Equivalently, combining Eqs.~\eqref{eq:normalized_coords} and \eqref{eq:intrinsics}, the direct projection of a point in the camera reference frame to pixel coordinates can be expressed as follows:

\begin{equation}
	\lambda
	\begin{bmatrix}
		u\\v\\1
	\end{bmatrix}
	=
	K
	\begin{bmatrix}
		X_c\\Y_c\\Z_c
	\end{bmatrix},
	\qquad \lambda=Z_c,
	\label{eq:pinhole_projection}
\end{equation}

Thus, this subsection is directly connected to the formulation used later in \textit{Perspective-n-Point}.

Real lenses introduce geometric distortions that deviate from the ideal projection of the \textit{pinhole} model. In practical applications, these distortions are often modeled by radial and tangential terms, estimated through calibration \cite{zhang2000calib}. The presence of unmodeled or even poorly calibrated distortion systematically shifts image observations, potentially affecting mainly corner localization and, consequently, degrading pose estimation based on reprojection.

For uncertainty analysis, it is common to assume that corner observations in the image are affected by approximately Gaussian additive noise in pixels. Thus, for each corner $i$ one observes:

\begin{equation}
	\hat{\mathbf{u}}_i=\mathbf{u}_i+\boldsymbol{\epsilon}_i,
	\qquad
	\boldsymbol{\epsilon}_i\sim\mathcal{N}(\mathbf{0},\Sigma_u),
	\label{eq:pixel_noise}
\end{equation}

\noindent in which $\mathbf{u}_i=[u_i\;v_i]^\top$ represents the ideal projection and $\hat{\mathbf{u}}_i$ the measurement in the image. This hypothesis justifies formulating pose estimation as a least-squares problem via reprojection error, since under Gaussian noise the minimization of the sum of squared errors coincides with maximum likelihood estimation \cite{hartley2004mv}.


In this way, it is assumed that the robot camera is calibrated, such that $K$ and the distortion coefficients are known. Corner detections obtained from AprilTags are treated as pixel observations.


\subsubsection{Pose by PnP}
\label{subsubsec:pnp_pose}

Once the four corners of the marker are obtained and its identifier is validated, the pose of the marker with respect to the camera can be estimated because the AprilTag has known planar geometry and defined physical dimensions. In the detector proposed by Olson, the geometry observed in the image is initially related to the marker plane through a homography; the author describes that the method computes the $3\times 3$ homography matrix and that it is obtained by the DLT (Direct Linear Transform) algorithm \cite{olson2011tags}. However, to recover position and orientation in metric units, the homography alone is not sufficient; pose estimation requires additional information, specifically the camera intrinsic parameters and the physical size of the marker \cite{olson2011tags}.

Modeling the camera using the pinhole model, the relationship between the known 3D points, which are the marker corners in its reference frame, and their 2D observations in the image can be written by Equation~\eqref{eq:pinhole_pnp}. In this formulation, each marker corner $\mathbf{X}_i$ is transformed to the camera reference frame by a rotation ${}^{\text{cam}}\!R_{\text{tag}}$ and a translation ${}^{\text{cam}}\!t_{\text{tag}}$, and then projected onto the image plane via the intrinsic matrix $K$:

\begin{equation}
	\lambda_i
	\begin{bmatrix}
		u_i\\ v_i\\ 1
	\end{bmatrix}
	=
	K
	\Big(
	{}^{\text{cam}}\!R_{\text{tag}}\mathbf{X}_i
	+
	{}^{\text{cam}}\!t_{\text{tag}}
	\Big),
	\label{eq:pinhole_pnp}
\end{equation}
\noindent in which $\lambda_i$ is a scale factor, $\mathbf{u}_i=[u_i\;v_i]^\top$ represents the corner observation in the image, and $\mathbf{X}_i$ represents the corresponding corner in the marker reference frame.

Thus, pose estimation by PnP can then be interpreted as the problem of finding the parameters ${}^{\text{cam}}\!R_{\text{tag}}$ and ${}^{\text{cam}}\!t_{\text{tag}}$ that best explain the observations, minimizing the reprojection error. Equation~\eqref{eq:reproj_refine} formalizes this idea by defining a cost function based on the difference between the observed corners $\mathbf{u}_i$ and the reprojected corners $\hat{\mathbf{u}}_i$, obtained by projecting the points transformed by the geometric model:
\begin{equation}
	\label{eq:reproj_refine}
	\begin{aligned}
		\min_{{}^{\text{cam}}\!R_{\text{tag}}\in SO(3),\,{}^{\text{cam}}\!t_{\text{tag}}\in\mathbb{R}^3}
		\;\; \sum_{i=1}^{4}
		\left\|
		\mathbf{u}_i
		-
		\pi\!\left(
		K\left(
		{}^{\text{cam}}\!R_{\text{tag}}\,\mathbf{X}_i
		+
		{}^{\text{cam}}\!t_{\text{tag}}
		\right)
		\right)
		\right\|^2
		\\[2pt]
		=
		\min_{{}^{\text{cam}}\!R_{\text{tag}},\,{}^{\text{cam}}\!t_{\text{tag}}}
		\;\; \sum_{i=1}^{4}
		\left\|
		\mathbf{u}_i - \hat{\mathbf{u}}_i
		\right\|^2,
		\quad
		\hat{\mathbf{u}}_i=\pi\!\left(
		K\left(
		{}^{\text{cam}}\!R_{\text{tag}}\,\mathbf{X}_i
		+
		{}^{\text{cam}}\!t_{\text{tag}}
		\right)
		\right).
	\end{aligned}
\end{equation}

\noindent
Thus, in Eq.~\eqref{eq:reproj_refine}, the function $\pi(\cdot)$ represents the perspective projection, defined as
$\pi\!\left([x\;y\;z]^\top\right)=\left[x/z\;\;y/z\right]^\top$.
This formulation highlights that the quality of the estimated pose depends directly on the accuracy of corner detection $\mathbf{u}_i$, intrinsic calibration $K$, and the geometric validity of the marker. Therefore, corner quantization errors, motion blur, illumination variations, or inconsistencies in detection degrade the reprojection error and, consequently, the geometric consistency.


\subsection{AprilTag}
\label{subsec:apriltag_theory}

AprilTags are planar fiducial markers widely used as \emph{landmarks} in robotics and computer vision, as they allow recovering the marker identity and estimating its relative pose with respect to the camera from a single image \cite{olson2011tags,wang2016iros}. However, unlike 2D codes designed for high data capacity, the design of AprilTags prioritizes perceptual robustness and a low false positive rate. Therefore, this robustness results from the combination of a high-contrast outer border, suitable for geometric extraction under perspective, and an internal \emph{payload} constructed as a finite family of binary codes with Hamming distance validation \cite{olson2011tags,wang2016iros}.

Thus, Figure~\ref{fig:apriltag_structure} illustrates this functional separation: the border facilitates quadrilateral detection and accurate estimation of the four corners in the image, while the internal region encodes the identifier in a redundant and verifiable manner. In general terms, detection can be understood as a two-stage process, locating geometrically consistent candidates and rectifying the internal region to decode the \emph{payload} \cite{olson2011tags}. After ID validation, the observed corners provide 2D/3D correspondences with the known square geometry, which allows estimating the relative pose of the marker via the camera projective model.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.35\linewidth]{Images/tag_size_48h12.png}
	\caption{Structure of an AprilTag.}
	\label{fig:apriltag_structure}
\end{figure}

In this way, each detection provides the marker identifier and a rigid transformation associated with its relative pose, usually expressed as ${}^{\text{cam}}\!T_{\text{tag}}\in SE(3)$. This transformation is obtained from the correspondences between the corners in the image and the known corners in the marker reference frame, and can be refined by minimizing the reprojection error. Likewise, Figure~\ref{fig:apriltag_pipeline} shows a typical example of detector visualization, in which the estimated quadrilateral, the decoded identifier, and the marker reference frame axes are overlaid on the image.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.55\linewidth]{Images/repre_exemple.png}
	\caption{Example of output from an AprilTag detector.}
	\label{fig:apriltag_pipeline}
\end{figure}


\subsubsection{Hamming distance}
\label{subsubsec:hamming_fp}

At this stage, AprilTag decoding is deliberately conservative, as it aims to reject ambiguous patterns and reduce false positives. Therefore, the central principle consists of comparing the observed \emph{codeword} with a dictionary of valid codes and accepting a detection only when there is a sufficiently close match.

In the original work, the decision rule is described by a Hamming threshold, in which "If the best match has a Hamming distance less than the user-specified threshold, a detection is reported" \cite{olson2011tags}. This mechanism makes explicit the trade-off between \emph{recall} and false positive rate: more permissive thresholds increase the chance of accepting degraded readings, but also raise the risk of incorrect detections.

In this case, given two bit strings $a,b\in\{0,1\}^n$, the Hamming distance is defined as
\begin{equation}
	d_H(a,b)=\sum_{k=1}^{n}\mathbb{I}\{a_k\neq b_k\},
	\label{eq:hamming}
\end{equation}
in which $\mathbb{I}\{\cdot\}$ is the indicator function. In AprilTags, however, the orientation of the marker in the image is unknown \emph{a priori}, which requires validation to consider all possible discrete rotations of the pattern. For this, the minimum Hamming distance under rotations is defined as
\begin{equation}
	d_H^{\mathrm{rot}}(a,b)=
	\min_{r\in\{0,1,2,3\}}
	d_H\!\big(a,\rho^{r}(b)\big),
	\label{eq:hamming_rot}
\end{equation}
in which $\rho^{r}(\cdot)$ represents the rotation of the \emph{codeword} in multiples of $90^\circ$.

In the case of AprilTag 2, the authors highlight that payload generation preserves a minimum separation between valid codes by explicitly considering all possible rotations, stating that the system is built by "guaranteeing a minimum Hamming distance between tags under all possible rotations" \cite{wang2016iros}. In this way, the threshold applied to Equation~\eqref{eq:hamming_rot} acts as an explicit reliability criterion, reducing the probability of false positives even under noise, partial occlusions, or image degradation. Therefore, a conservative Hamming distance threshold was adopted, prioritizing a low false positive rate at the expense of more permissive readings.


Thus, AprilTag~3 extends the system by allowing the generation of families with flexible \emph{layouts}, while preserving principles associated with low false positive rates. Krogius \emph{et al.} highlight as a central contribution "a flexible layout system whereby users can generate sets of tags with the data bits arranged in a specified shape" \cite{krogius2019iros}. Therefore, to maintain reliability while expanding the pattern space, the authors introduce "a complexity metric applicable to diverse tag layouts which we use to generate tags with low false positive rates" \cite{krogius2019iros}. Regardless of the chosen family, the projection and pose estimation equations remain valid, as they depend on the planar geometry of the marker and the camera model, and not on the specific binary pattern employed.

\section{Methodology}

\subsection{Pipeline validation}
\label{subsec:rviz_validation}

Before integration into simulation in Gazebo Ignition, a preliminary validation stage was carried out with the objective of verifying the geometric consistency of the proposed pipeline. In this case, this validation had an exclusively qualitative character and was conducted in the RViz environment, which allowed inspection of the reference frames, axes, and transformations involved in the problem.

At this stage, a simple script was developed responsible for publishing the main geometric elements of the system in RViz, the camera and the tags, without involving robot dynamics or control. The objective was to ensure that the conventions adopted for pose representation and transformation composition in $SE(3)$ were correct before introducing additional simulation effects\cite{ros2tf2humble}.

In particular, the script allowed simultaneous visualization of the nominal camera trajectory along a closed circular path, defined directly in the global reference frame, as well as the poses of the fiducial markers also expressed in the anchor reference frame, which was defined as 36h11:0. In addition, the \textit{frames} associated with the camera and the tags were represented by RViz TFs.

Therefore, this visualization made it possible to confirm the correct orientation of the camera and fiducial marker axes, as well as the consistency of the rigid transformations in $SE(3)$ used to express relative and absolute poses.


Thus, immediately after the basic geometric validation, a second stage of qualitative verification was carried out, in which controlled synthetic noise was introduced into the camera and marker poses. The objective of this stage was to visually observe the behavior of the pipeline under small perturbations, simulating typical inaccuracies of visual estimation of AprilTags.

Under this condition, RViz allowed visualization of whether pose compositions remained geometrically plausible over time, that is, whether small perturbations resulted in smooth and consistent frame displacements, without causing discontinuities, axis inversions, or degenerate behaviors.

Therefore, with the consistency of the geometric model verified in these preliminary stages, the same set of transformations, reference frame conventions, and assumptions began to be used in the simulation environment in Gazebo Ignition.



\subsection{Teach Phase}
\label{subsec:teach_phase}

At this stage, the term \emph{Teach Phase} refers to the process of systematic acquisition of geometric observations of the environment while the robot traverses a known circuit containing multiple fiducial markers. Teaching the robot, in this context, means providing sufficient information to geometrically describe the environment through the transformations associated with the markers observed along the path, without yet performing any form of global optimization or error correction.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{Images/figmet01.png}
	\caption{Circuit in Gazebo Ignition and distribution of AprilTags.}
	\label{fig:teach_env_layout}
\end{figure}

Thus, the simulation environment contains a closed circuit with 24 AprilTags distributed along the trajectory, as shown in Figure~\ref{fig:teach_env_layout}. Among these, only one is treated as the absolute reference of the system. The pose of the \emph{tag zero} is known \emph{a priori} and acts as the map anchor, defining the global reference frame. The physical size of the markers, the camera intrinsic parameters, and its calibration are assumed to be known from the beginning of the experiment. In this way, the global map is initialized exclusively from the anchor, with no prior knowledge of the position of the remaining tags.

In this case, at the beginning of the Teach Phase, the system does not know the pose of the other 23 tags, the trajectory to be traversed, nor the global pose of the camera over time. Moreover, these elements are progressively inferred from the visual observations recorded during motion. In this manner, at each instant, the robot observes a subset of the markers present in the environment, and it is imposed as an operational criterion that, whenever possible, at least two tags are simultaneously within the camera field of view. This condition favors geometric consistency between successive observations and allows establishing spatial relationships between different markers.

Furthermore, during the Teach Phase, the robot is manually guided by the operator along the circuit. The motion is conducted with approximately constant velocity and without abrupt accelerations, in order to preserve the quality of visual observations and reduce degradations associated with motion blur. Thus, the trajectory is closed, starting and ending in the region of the anchor tag, so that the global reference is observed both at the beginning and at the end of execution.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{Images/teach_axes_composed_relaxed.png}
	\caption{Reference frames used: \textit{world}, \textit{Limo}, and \textit{tag}.}
	\label{fig:frames_world_cam_tag}
\end{figure}


Thus, as the robot moves, each marker detection provides the rigid transformation between the camera reference frame and the reference frame of the observed tag, that is, ${}^{\text{cam}}\!T_{\text{tag}}(t)$. When the anchor tag is observed, the global pose of the camera can be estimated by composition in $SE(3)$:
\begin{equation}
	{}^{\text{world}}\!T_{\text{cam}}(t)
	=
	{}^{\text{world}}\!T_{\text{tag}_0}\;
	\left({}^{\text{cam}}\!T_{\text{tag}_0}(t)\right)^{-1}.
	\label{eq:teach_worldTcam_from_anchor}
\end{equation}
In this way, for the remaining tags, the recorded observations provide relative relationships over time, which will later be used for offline optimization of their trajectory.

Figure~\ref{fig:teach_pipeline_tikz} summarizes the flow executed in the Teach Phase, highlighting detection acquisition, pose estimation, and temporal logging in \textit{rosbag}.

% --- TIKZ DIAGRAM (LuaLaTeX) of the Teach pipeline (improved and colored version)
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[
		font=\small,
		node distance=12mm and 10mm,
		>=Latex,
		% styles
		io/.style={
			trapezium, trapezium left angle=70, trapezium right angle=110,
			draw=black!60, fill=blue!10,
			align=center, inner sep=6pt,
			minimum width=28mm, minimum height=11mm
		},
		proc/.style={
			rectangle, rounded corners=2.5mm,
			draw=black!60, fill=gray!10,
			align=center, inner sep=6pt,
			minimum width=32mm, minimum height=11mm
		},
		pose/.style={
			rectangle, rounded corners=2.5mm,
			draw=black!60, fill=green!12,
			align=center, inner sep=6pt,
			minimum width=36mm, minimum height=11mm
		},
		aux/.style={
			rectangle, rounded corners=2.5mm,
			draw=black!60, fill=orange!15,
			align=center, inner sep=6pt,
			minimum width=36mm, minimum height=11mm
		},
		store/.style={
			trapezium, trapezium left angle=70, trapezium right angle=110,
			draw=black!60, fill=purple!12,
			align=center, inner sep=6pt,
			minimum width=28mm, minimum height=11mm
		},
		arrow/.style={->, thick, draw=black!70},
		note/.style={
			draw=black!55, dashed, rounded corners=2mm,
			fill=yellow!12,
			align=left, inner sep=7pt,
			text width=62mm
		}
		]
		
		% main nodes (top row)
		\node[io]   (cam)    {RGB Camera\\(frames)};
		\node[proc, right=of cam] (detect) {AprilTag\\Detection};
		\node[pose, right=of detect] (pnp) {Pose estimation\\${}^{\text{cam}}\!T_{\text{tag}}(t)$};
		\node[store, right=of pnp] (log) {Logging\\\textit{rosbag}};
		
		% lower branch
		\node[aux, below=12mm of pnp] (se3) {$SE(3)$ composition\\(anchor visible)};
		\node[pose, right=of se3] (worldcam) {Estimate\\${}^{\text{world}}\!T_{\text{cam}}(t)$};
		
		% main connections
		\draw[arrow] (cam) -- (detect);
		\draw[arrow] (detect) -- (pnp);
		\draw[arrow] (pnp) -- (log);
		
		% lower branch connections
		\draw[arrow] (pnp) -- (se3);
		\draw[arrow] (se3) -- (worldcam);
		
		% worldcam -> log connection with clean elbow
		\draw[arrow] (worldcam.east) -- ++(6mm,0) |- ([yshift=-4mm]log.south) -- (log.south);
		
		
		% (optional) light frame around the pipeline
		\node[draw=black!25, rounded corners=3mm, fit=(cam)(log)(worldcam), inner sep=7mm] {};
		
	\end{tikzpicture}
	\caption{Processing flow in the Teach Phase.}
	\label{fig:teach_pipeline_tikz}
\end{figure}



It is worth noting that all observations acquired during the Teach Phase are stored in log files in the \textit{rosbag} format. In this way, these records include tag detections, the associated geometric transformations, and the temporal information necessary to reconstruct the sequence of events; in summary, the teach phase collects all camera and tag TFs, as can be seen in the pseudocode in Algorithm~\ref{alg:teach_phase}, to be later optimized and used in the replay phase.



\begin{algorithm}[H]
	\label{subsubsec:teach_pseudocode}
	\caption{Teach Phase: acquisition and logging of geometric observations}
	\label{alg:teach_phase}
	\DontPrintSemicolon
	\KwIn{Known anchor pose ${}^{\text{world}}\!T_{\text{tag}_0}$; intrinsics $K$; physical size of the tags}
	\KwOut{Temporal log: detections, ${}^{\text{cam}}\!T_{\text{tag}}(t)$, and (when possible) ${}^{\text{world}}\!T_{\text{cam}}(t)$}
	
	Initialize \textit{rosbag} and storage structures\;
	Define global reference frame \textit{world} from the anchor (tag 0)\;
	
	\While{Teach Phase active (robot manually guided)}{
		Receive camera frame at time $t$\;
		Detect set of visible tags $\mathcal{S}(t)$ (IDs and corners)\;
		
		\If{$|\mathcal{S}(t)| < 2$}{
			Mark state as ``low observability'' (without aborting)\;
		}
		
		\ForEach{tag $j \in \mathcal{S}(t)$}{
			Estimate relative pose ${}^{\text{cam}}\!T_{\text{tag}_j}(t)$ from observed corners and known geometry\;
			Log $\big(t,\; j,\; {}^{\text{cam}}\!T_{\text{tag}_j}(t)\big)$ to the \textit{rosbag}\;
		}
		
		\If{anchor tag $0 \in \mathcal{S}(t)$}{
			Compute ${}^{\text{world}}\!T_{\text{cam}}(t)$ using composition in $SE(3)$:\;
			\hspace{6mm}${}^{\text{world}}\!T_{\text{cam}}(t) \leftarrow {}^{\text{world}}\!T_{\text{tag}_0}\left({}^{\text{cam}}\!T_{\text{tag}_0}(t)\right)^{-1}$\;
			Log $\big(t,\; {}^{\text{world}}\!T_{\text{cam}}(t)\big)$ to the \textit{rosbag}\;
		}
		
		Update counters/statistics (e.g., number of frames, number of detections)\;
	}
	
	Finalize and save the \textit{rosbag}\;
\end{algorithm}

\section{Results}
\subsection{Results in RViz}
\label{subsec:results_rviz}

This subsection presents the results of the \textit{Teach Phase} in a controlled scenario, visualized in RViz, before the application of any optimization stage. The objective here is to verify the geometric consistency of the pipeline: the estimated trajectory must be compatible with the distribution of the markers and, since it is a closed circuit, the shape of the path must be preserved.


\begin{figure}[h!]
	\centering
	\includegraphics[
	width=0.4\linewidth,
	trim=7cm 0 7cm 0, % left, bottom, right, top
	clip
	]{Images/circle_dense_truth.png}
	\caption{Reference trajectory (\textit{ground-truth}) in the $XY$ plane.}
	\label{fig:rviz_truth_xy}
\end{figure}


Thus, Figure~\ref{fig:rviz_truth_xy} shows the reference trajectory (\textit{ground-truth}) in the $XY$ plane, used as a basis for comparison. Next, Figure~\ref{fig:rviz_estimated_xy} presents the estimated trajectory obtained from successive compositions with the anchor and the observed transformations. In this case, it is observed that the estimate preserves the general shape of the circuit and maintains spatial coherence with the arrangement of the tags, indicating that the reference frame conventions and the composition in $SE(3)$ are correct. Small local differences are expected, since each observation contributes directly to the chaining process, and local errors propagate along the sequence.

\begin{figure}[H]
	\centering
	
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[
		width=\linewidth,
		trim=7cm 0 7cm 0,
		clip
		]{Images/circle_dense_estimated.png}
		\caption{Estimated trajectory in the $XY$ plane.}
		\label{fig:rviz_estimated_xy}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[
		width=\linewidth,
		trim=5.5cm 0 5.5cm 0,
		clip
		]{Images/circle_dense_tf.png}
		\caption{Visualization of the \textit{frames} (\textit{TF}).}
		\label{fig:rviz_tf_xy}
	\end{subfigure}
	
	\caption{Results in the $XY$ plane.}
	\label{fig:xy_lado_a_lado}
\end{figure}


To complement the interpretation, Figure~\ref{fig:rviz_tf_xy} presents the \textit{TF frames} associated with the tags in the $XY$ plane, together with samples of the camera pose along the trajectory. This visualization is particularly useful for identifying axis inversions or convention inconsistencies.




\subsection{Simulation in Gazebo Ignition}
\label{subsec:results_gazebo}

For the Gazebo Ignition simulation, unlike the controlled case in RViz, the system operates under conditions closer to real execution: the robot moves in the simulated world, the camera produces images at discrete time steps, and pose estimation depends directly on the quality of AprilTag detections. Thus, effects such as temporal discretization, perspective variations, noise in corner estimation, and small dynamic inconsistencies are reflected in the inferred trajectories and poses.

Therefore, Figure~\ref{fig:gazebo_estimated_xy} shows the estimated camera trajectory in the $XY$ plane during the Teach Phase. It can be observed that the global shape of the circuit is preserved, indicating that the chaining of transformations maintains geometric consistency along the path. However, the trajectory exhibits local irregularities and segments with greater dispersion, reflecting the cumulative nature of error when no global refinement is applied.

\begin{figure}[H]
	\centering
	\includegraphics[
	width=0.5\linewidth,
	trim=5cm 0 5cm 0, % left, bottom, right, top
	clip
	]{Images/bag_camera_traj_estimated.png}
	\caption{Estimated camera trajectory in the $XY$ plane.}
	\label{fig:gazebo_estimated_xy}
\end{figure}


In this way, to evaluate how these uncertainties impact the local map of the markers, Figure~\ref{fig:gazebo_tags_compare_xy} compares the estimated tag positions with the reference positions in the simulated world. It is noted that some tags present more pronounced displacements, especially in regions where the trajectory includes abrupt variations or where detection quality is potentially lower. Thus, this behavior is expected in a purely observational Teach Phase: each local estimate contributes directly to the chaining process and, without fusion of multiple observations, the discrepancy tends to vary from tag to tag.

\begin{figure}[H]
	\centering
	\includegraphics[
	width=0.5\linewidth,
	trim=5cm 0 5cm 0, % left, bottom, right, top
	clip
	]{Images/bag_camera_traj_tags_compare.png}
	\caption{Comparison between estimated positions and reference positions of AprilTags in $XY$.}
	\label{fig:gazebo_tags_compare_xy}
\end{figure}


Finally, Figure~\ref{fig:gazebo_tf_xy} presents a visualization of the \textit{TF frames} of the tags and samples of the camera pose along the trajectory. This figure is useful for diagnosing reference frame consistency: even with noise and dispersion, the axes remain coherent and no systematic orientation inversions are observed.

\begin{figure}[H]
	\centering
	\includegraphics[
	width=0.5\linewidth,
	trim=4.5cm 0 4.8cm 0, % left, bottom, right, top
	clip
	]{Images/bag_camera_traj_tf.png}
	\caption{Visualization of the \textit{TF frames}.}
	\label{fig:gazebo_tf_xy}
\end{figure}


In summary, the simulation results highlight the expected behavior of a Teach Phase without optimization: the global structure of the circuit is preserved, but local errors progressively accumulate along the trajectory. These limitations directly motivate the introduction of the optimization stage, in which multiple observations of the same tag can be combined and the camera trajectory smoothed to reinforce the global consistency of the map.


\newpage

\section{Discussion}
\newpage

\section{Conclusion}



\newpage

\printbibliography

\end{document}

%*=============================================================================*
%* Fin du document LaTeX
%*=============================================================================*
